---
layout: post
title:  "Docker"
date:   2016-02-21 00:00:00 +0000
categories: [General]
tags: [docker, virtualization]
---

Before virtual machines you would install just one OS on one server hardware, where all of the systems resources (RAP, CPU...) would be dedicated to one server OS.  
Vith virtualization, instead of installing one OS, you install Hypervisor (like VMWare). Hypervisor can be looked at as kind of an OS that has only one goal, to help you divide server resources into multiple servers.

## How is Docker different from a normal virtual machine?

> Virtualization and virtual machines are virtualizing the  hardware, Docker virtualizes the OS.

A virtual machine emulates a physical computing environment, but requests for CPU, memory, hard disk, network and other hardware resources are managed by a virtualization layer which translates these requests to the underlying physical hardware.
In this context the VM is called as the `Guest` while the environment it runs on is called the `Host`

Docker was using LinuX Containers (`LXC`) earlier, but switched to runC (formerly known as libcontainer), that runs in the same operating system as its host. This allows it to share a lot of the host operating system resources. It also uses layered filesystems like `AuFS` and it manages the networking for you as well.

`AuFS` is a layered file system, so you can have a read only part, and a write part, and merge those together. This enables you to have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.

So let's say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With `LXC` and `AuFS` you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.  
A full virtualized system gets its own set of resources allocated to it, and does minimal sharing. You get more isolation, but it is much heavier (requires more resources).  

The advantage of Docker, besides the portability, is that you no longer need a virtual machine spun up for each and every app. With `LXC` you get less isolation, but they are more lightweight and require less resources. So you could easily run 1000's on a host.   
A full virtualized system usually takes minutes to start, `LXC` containers take seconds.  

There are pros and cons for each type of virtualized system. If you want full isolation with guaranteed resources, a full VM is the way to go. If you just want to isolate processes from each other and want to run a ton of them on a reasonably sized host, then `LXC` might be the way to go.

Understand, though, that containers are pretty dumb on their own, lacking functions for replication and scheduling or a robust way to handle packages and install services. For this Google (a long time consumer of Linux containers itself) released Kubernetes, a Docker container management and orchestration tool, to the open-source community. 


## What does it mean to "snapshot the OS". How does one do that without, well, making an image of the OS?
You start with a base image, and then make your changes, and commit those changes using docker, and it creates an image. This image contains only the differences from the base. When you want to run your image, you also need the base, and it layers your image on top of the base using a layered file system, in this case `AUFS`. `AUFS` merges the different layers together and you get what you want, and you just need to run it. You can keep adding more and more images (layers) and it will keep only saving the diffs. Since docker typically builds on top of ready-made images from a registry, you rarely have to "snapshot" the whole OS yourself.

In some places you conflate the OS with the kernel. All containers on a host run under the same kernel, but the rest of the OS files can be unique per container.  To clarify, the OS "userland" is a bunch files on disk. The kernel is the part that can be in disk OR ROM, OR passed in by `LXC` to a container, and first runs those files. It's also the part that changes little between linux distros. Because `LXC` can use any image (think folder) as the root of a new userland, you can have n shared instances of m distros, all running independently, but with each one writing to its own NON-shared location on disk. However, you can also snapshot any of those instances at any time, and use that as a new, shared starting point for other instances.


It might be helpful to understand how virtualization and containers work at low level. 


## How virtualization works at low level?
In this case VM manager takes over the CPU ring 0 (or the "root mode" in newer CPUs) and intercepts all privileged calls made by guest OS to create illusion that guest OS has its own hardware. Fun fact: Before 1998 it was thought to be impossible to achieve this in x86 architecture because there was no way to do this kind of interception. The folks at VMWare were the first who had an idea to rewrite the executable bytes in memory for privileged calls of guest OS to achieve this. The net effect is that virtualization allows you to run two completely different OS on same hardware. Each guest OS goes through all the process of bootstrapping, loading kernel etc. You can have very tight security, for example, guest OS can't get full access to host OS or other guests and mess things up.

## How containers works at low level?
Around 1996, people including some of the employees at Google implemented new kernel level  feature called namespaces. One of the function of OS is to allow sharing of global resources like network and disk to processes. What if these global resources were wrapped in namespaces so that they are visible only to those processes that runs in the same namespace? Say, you can get a chunk of disk and put that in namespace X and then processes running in namespace Y can't see or access it. Similarly, processes in namespace X can't access anything in memory that is allocated to namespace Y. Of course, processes in X can't see or talk to processes in namespace Y. This provides kind of virtualization and isolation for global resources. This is how docker works: Each container runs in its own namespace but uses exactly the same kernel as all other containers. The isolation happens because kernel knows the namespace that was assigned to the process and during API calls it makes sure that process can only access resources in its own namespace.
The limitations of containers vs VM should be obvious now: You can't run completely different OS in containers like in VMs. However you can run different distros of Linux because they do share the same kernel. Also unlike VM, you don't have to pre-allocate significant chunk of memory to VM because we are not running new copy of OS. 


[Source](https://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-virtual-machine/16048358#16048358)
